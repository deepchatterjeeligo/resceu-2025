{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Likelihood-free Inference using pytorch-lightning\n",
    "\n",
    "### RESCEU Summer School 2025\n",
    "\n",
    "#### Deep Chatterjee (deep1018@mit.edu), LIGO Lab & A3D3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Brief background on likelihood-free inference\n",
    "- Look at a simple problem - _Linear regression_\n",
    "  - Infer slope and intercept of a line given noisy data\n",
    "  - Work out the answer analytically\n",
    "  - Do the same using stochastic sampling - MCMC/nested sampling\n",
    "  - Do the same using posterior estimation via a normalizing flow using `pyro`\n",
    "- Introduce `pytorch-lightning` and do the same problem using distributed training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inference\n",
    "\n",
    "$$\\text{We have a model}\\;f(\\mathbf{\\Theta}) \\rightarrow \\text{Collect data}\\;\\mathbf{d} \\rightarrow \\text{infer signal parameters}\\;\\mathbf{\\Theta}\\;$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Terminology\n",
    "  - **Signal**: True model i.e., data in absence of noise\n",
    "  - **Data**: Signal + Noise\n",
    "  - $\\mathbf{d} = f(\\mathbf{\\Theta}) + n$\n",
    "- _Noise_ in experiments causes measurement _uncertainties_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Noise $\\rightarrow$ probability distribution of data, $\\mathbf{d}$ for the same parameters, $\\mathbf{\\Theta}$.\n",
    "\n",
    "$$ \\text{Likelihood}\\;L \\equiv p(\\mathbf{d}\\vert\\mathbf{\\Theta}) $$\n",
    "\n",
    "- As noise $\\rightarrow 0$, $L \\rightarrow \\delta[\\mathbf{d} - f(\\mathbf{\\Theta})]$.\n",
    "- In bayesian inference, we want the posterior,\n",
    "\n",
    "$$p(\\mathbf{\\Theta}\\vert\\mathbf{d}) = \\frac{p(\\mathbf{d}\\vert\\mathbf{\\Theta})\\;p(\\mathbf{\\Theta})}{p(\\mathbf{d})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Generally done using stochastic sampling techniques (MCMC, nested sampling)\n",
    "  - Propose $\\mathbf{\\Theta}_i \\sim p(\\mathbf{\\Theta})$\n",
    "  - Generate signal $f(\\mathbf{\\Theta}_i)$\n",
    "  - Compute likelihood $L_i \\rightarrow$ Accept/Reject proposed point.\n",
    "  - Repeat..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear model\n",
    "\n",
    "\\begin{align}\n",
    "y_i =  \\underbrace{mx_i + c}_{\\text{true model }f(x_i)} + \\epsilon_i.\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"./linear-regression-example.png\" style=\"display:block; margin:auto\" width=\"400\" height=\"300\">\n",
    "\n",
    "- We want to estimate slope and intercept, $\\{m, c\\}$\n",
    "- Data is noisy. Assume $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Likelihood\n",
    "\n",
    "- The noise term, $\\epsilon\\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "\\begin{align}\n",
    "  L(\\{y_i\\} \\vert m, c, \\sigma)  &\\propto \\exp\\left[-\\frac{1}{2\\sigma^2}\\sum_i\\left(y_i - mx_i - c\\right)^2\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Maximize the likelihood\n",
    "\n",
    "\\begin{align}\n",
    "  \\frac{\\partial L}{\\partial m} = \\frac{\\partial L}{\\partial c} = 0\n",
    "\\end{align}\n",
    "\n",
    "- Least square answer\n",
    "\n",
    "\\begin{align}\n",
    "  \\hat{m} = \\frac{\\sum x_iy_i}{\\sum x_i^2}\\;\\;\\;;\\;\\; \\hat{c} = \\frac{\\sum y_i}{n}.\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Re-express likelihood around maximum $\\{\\hat{m},  \\hat{c}\\}$\n",
    "\n",
    "\\begin{align}\n",
    "   L(\\{y_i\\} \\vert m, c, \\sigma) &\\propto \\exp{\\left[-\\frac{\\sum x_i^2}{2\\sigma^2}\\left(m - \\hat{m}\\right)^2\\right]} \\times\n",
    "  \\exp{\\left[-\\frac{n}{2\\sigma^2}\\left(c - \\hat{c}\\right)^2\\right]}.\n",
    "\\end{align}\n",
    "\n",
    "- The likelihood is a gaussian around the least square values.\n",
    "- Corresponding widths are $\\Delta m \\sim \\left(\\sigma/\\sqrt{\\sum x_i^2}\\right)$ and $\\Delta c \\sim \\left(\\sigma/\\sqrt{n}\\right)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Posterior\n",
    "\n",
    "\\begin{align}\n",
    "p(m, c, \\sigma \\vert \\{y_i\\}) \\propto L(\\{y_i\\} \\vert m, c, \\sigma)\\;p(m)\\;p(c)\\;p(\\sigma)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume uninformative priors on parameters, $p(m)\\;,\\; p(c) \\sim 1$.\n",
    "- Assume $\\sigma$ is known i.e. $p(\\sigma') = \\delta(\\sigma - \\sigma')$.\n",
    "- The posterior is also a gaussian (same as the likelihood in this example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Marginals\n",
    "\n",
    "\\begin{align}\n",
    "  p(m\\vert \\{y_i\\}) &\\propto \n",
    "  \\int L(\\{y_i\\}\\vert m, c, \\sigma')\\;\\underbrace{p(m)\\;p(c)}_{\\sim 1}\\;\\overbrace{p(\\sigma)}^{\\sim \\delta(\\sigma - \\sigma')} dc\\;d\\sigma' \\\\\n",
    "  &\\propto \\exp{\\left[-\\frac{\\sum x_i^2}{2\\sigma^2}\\left(m - \\hat{m}\\right)^2\\right]}.\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "  p(c\\vert \\{y_i\\}) &\\propto \n",
    "  \\int L(\\{y_i\\}\\vert m, c, \\sigma')\\;{p(m)\\;p(c)}\\;{p(\\sigma)}\\;dm\\;d\\sigma' \\\\\n",
    "  &\\propto \\exp{\\left[-\\frac{n}{2\\sigma^2}\\left(c - \\hat{c}\\right)^2\\right]}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import pytorch as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, m, c):\n",
    "    return m*x + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Putting some numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "injection_parameters = dict(m=0.8, c=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_points = 50\n",
    "x = np.linspace(-4, 4, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sigma = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = linear_model(x, **injection_parameters) + np.random.normal(0, sigma, x.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, data, 'o', label='$\\\\{x_i, y_i\\\\}$')\n",
    "ax.plot(x, linear_model(x, **injection_parameters), '--r', label='f(x)')\n",
    "ax.legend()\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "m_hat = (x * data).sum() / (x * x).sum()\n",
    "c_hat = data.sum() / num_points\n",
    "\n",
    "delta_m = sigma/np.sqrt(np.sum((x)**2))\n",
    "delta_c = sigma * np.sqrt(1/num_points)\n",
    "\n",
    "print(\"Expected m = {:.3f} +/- {:.3f}\".format(m_hat, delta_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Expected c = {:.3f} +/- {:.3f}\".format(c_hat, delta_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bilby\n",
    "from bilby.core.likelihood import GaussianLikelihood\n",
    "from bilby.core.prior import Uniform, DeltaFunction\n",
    "\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "priors = dict()\n",
    "\n",
    "priors['m'] = Uniform(-3, 3, name='m', latex_label='m')\n",
    "priors['c'] = Uniform(-3, 3, name='c', latex_label='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "log_l = GaussianLikelihood(x, data, linear_model, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    result = bilby.run_sampler(\n",
    "        likelihood=log_l, priors=priors, sampler='dynesty',\n",
    "        nlive=500, npool=4, save=False, clean=True,\n",
    "        injection_parameters=injection_parameters,\n",
    "        outdir='./linear_regression',\n",
    "        label='linear_regression'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result from nested sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result.plot_corner(priors=True, quantiles=(0.16, 0.84))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Same problem using likelihood-free inference\n",
    "By posterior estimation using a normalizing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can simulate signals\n",
    "  - Given $\\mathbf{\\Theta}_i \\sim p(\\mathbf{\\Theta}) \\rightarrow$ generate signal $f(\\mathbf{\\Theta}_i)$\n",
    "  - Add instrument noise $f(\\mathbf{\\Theta}_i) + n \\rightarrow \\mathbf{d}_i$\n",
    "- We can \"easily\" get pairs $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\}$\n",
    "- From $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\} \\rightarrow p(\\mathbf{\\Theta}, \\mathbf{d}), p(\\mathbf{\\Theta}\\vert \\mathbf{d}), p(\\mathbf{d}\\vert\\mathbf{\\Theta})$\n",
    "- Here, we are interested in the posterior estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The distributions are complex\n",
    "- A solution is to use a Normalizing flow\n",
    "  - Contains a **learnable transform** (a trainable neural network)\n",
    "  - A **base distribution** (often taken to be normal)\n",
    "- Here we use a affine-autogressive transform from `pyro`\n",
    "- A standard normal base distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def live_plot_samples(samples, truth):\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    figure = corner.corner(\n",
    "        samples.numpy(), quantiles=[0.16, 0.5, 0.84],\n",
    "        show_titles=True, labels=[\"m\", \"c\"],\n",
    "        truth=truth\n",
    "    )\n",
    "\n",
    "    corner.overplot_lines(figure, truth, color=\"C1\")\n",
    "    corner.overplot_points(figure, truth[None], marker=\"s\", color=\"C1\")\n",
    "\n",
    "\n",
    "def live_plot_bilby_result(result, **kwargs):\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    result.plot_corner(priors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(m=None, c=None, num_points=1):\n",
    "    \"\"\"Sample m, c and return a batch of data with noise\"\"\"\n",
    "    m = priors['m'].sample() if m is None else m\n",
    "    c = priors['c'].sample() if c is None else c\n",
    "    x = np.linspace(-4, 4, num_points)\n",
    "    y = m*x + c\n",
    "    y += sigma*np.random.normal(size=x.size)\n",
    "\n",
    "    return x, y, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Generate simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "num_simulations = 40000\n",
    "theta_vals = []\n",
    "data_vals = []\n",
    "for ii in range(num_simulations):\n",
    "    x_val, y_val, m_val, c_val = get_data(num_points=num_points)\n",
    "    data_vals.append(y_val)\n",
    "    theta_vals.append([m_val, c_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "theta_vals = torch.from_numpy(np.array(theta_vals)).to(torch.float32)\n",
    "data_vals = torch.from_numpy(np.array(data_vals)).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    def __len__(self):\n",
    "        return num_simulations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        return theta_vals[idx], data_vals[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_set_size = int(0.8 * num_simulations)\n",
    "val_set_size = int(0.1 * num_simulations)\n",
    "test_set_size = int(0.1 * num_simulations)\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(\n",
    "    dataset, [train_set_size, val_set_size, test_set_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 40\n",
    "TEST_BATCH_SIZE = 1\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyro.nn import ConditionalAutoRegressiveNN\n",
    "from pyro.distributions import ConditionalTransformedDistribution\n",
    "from pyro.distributions.transforms import ConditionalAffineAutoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MADE in Pytorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quick recap of terminology\n",
    "- **Dataset**: the entire dataset, tuples of $\\{\\mathbf{\\Theta}_i, \\mathbf{d}_i\\}$\n",
    "- **Dataloader**: partition of dataset into batches i.e. gives a batch of data\n",
    "- **Training loop**: One epoch\n",
    "  - Push a batch of data\n",
    "  - Calculate loss\n",
    "  - Compute all gradients\n",
    "  - Change all weights based on gradients\n",
    "  - Repeats until all batches are done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pytorch-lightning removes boilerplate code\n",
    "- `torch.nn.Module` $\\rightarrow$ `pl.LightningModule`\n",
    "- Provides methods: `training_step`, `validation_step`, `test_step`, `configure_optimizers`\n",
    "- No need of explicit training loop\n",
    "- Automatic checkpoints, several options for logging\n",
    "- Easily scales to multi-GPU distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import corner\n",
    "\n",
    "def cast_as_bilby_result(samples, truth):\n",
    "    injections = dict.fromkeys(injection_parameters)\n",
    "    injections['m'] = float(truth.numpy()[0])\n",
    "    injections['c'] = float(truth.numpy()[1])\n",
    "\n",
    "    posterior = dict.fromkeys(injection_parameters)\n",
    "    samples_numpy = samples.numpy()\n",
    "    posterior['m'] = samples_numpy.T[0].flatten()\n",
    "    posterior['c'] = samples_numpy.T[1].flatten()\n",
    "    posterior = pd.DataFrame(posterior)\n",
    "    \n",
    "    return bilby.result.Result(\n",
    "        label=\"test_data\",\n",
    "        injection_parameters=injections,\n",
    "        posterior=posterior,\n",
    "        search_parameter_keys=list(injections.keys()),\n",
    "        priors=priors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MADE(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        context_dim,\n",
    "        hidden_dim,\n",
    "        learning_rate: float = LR,\n",
    "        batch_size: int = TRAIN_BATCH_SIZE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))\n",
    "        self.hypernet = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims)\n",
    "        self.transform = ConditionalAffineAutoregressive(self.hypernet)\n",
    "        self.flow = ConditionalTransformedDistribution(self.base_dist, [self.transform])\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def log_prob(self, theta, data):\n",
    "        return self.flow.condition(data).log_prob(theta).mean()\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        loss = - self.log_prob(theta, data)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        loss = - self.log_prob(theta, data)\n",
    "        self.log(\"valid_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        theta, data = batch\n",
    "        samples = self.flow.condition(data).sample([2000])\n",
    "        res = cast_as_bilby_result(samples, theta[0])\n",
    "        self.test_results.append(res)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        parameters = self.transform.parameters()\n",
    "        optimizer = torch.optim.AdamW(parameters, self.learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            self.learning_rate,\n",
    "            pct_start=0.1,\n",
    "            total_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        scheduler_config = dict(scheduler=scheduler, interval=\"step\")\n",
    "        return dict(optimizer=optimizer, lr_scheduler=scheduler_config)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            test_data,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class PPPlotCallback(pl.Callback):\n",
    "    def on_test_start(self, trainer, pl_module):\n",
    "        pl_module.test_results = []\n",
    "\n",
    "    def on_test_end(self, trainer, pl_module):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            bilby.result.make_pp_plot(pl_module.test_results, save=False, keys=['m', 'c'])\n",
    "        pl_module.test_results.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "context_dim = num_points\n",
    "hidden_dims = [5*input_dim, 5*input_dim]\n",
    "\n",
    "model = MADE(input_dim, context_dim, hidden_dims, batch_size=40, learning_rate=5e-3)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    log_every_n_steps=100,\n",
    "    logger=pl.loggers.CSVLogger(\"logs\", name=\"made-expt\"),\n",
    "    callbacks=[PPPlotCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Example posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for idx, (theta_test, data_test) in enumerate(test_data):\n",
    "    if idx == 5: break \n",
    "    with torch.no_grad():\n",
    "        samples = model.flow.condition(data_test).sample([1000])\n",
    "    live_plot_samples(samples, theta_test)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result from normalizing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    flow_samples = model.flow.condition(\n",
    "        torch.from_numpy(data).unsqueeze(0).to(dtype=torch.float32)\n",
    "    ).sample([1000])\n",
    "truth = np.array(list(injection_parameters.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "live_plot_samples(flow_samples, truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
