{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19daed5b-0157-440c-bc4f-0aec222a7902",
   "metadata": {},
   "source": [
    "# Learning a multimodal distribution\n",
    "\n",
    "Affine Autoregressive Transforms to learn the transform from a standard normal into a two-moon distribution. The code is light and can be run on a local laptop; no GPUs needed.\n",
    "\n",
    "- Transforms are based on `pyro`, which closely embraces the `torch.distributions` library.\n",
    "- `scikit-klearn` two-moon dataset is used.\n",
    "- `matplotlib` for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.distributions import Normal, TransformedDistribution\n",
    "\n",
    "from pyro.nn import AutoRegressiveNN\n",
    "from pyro.distributions.transforms import AffineAutoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d87bc52-13ad-4c1e-a2f9-731f96cca047",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, labels = datasets.make_moons(n_samples=1000, noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351f7c4-0a95-4012-a135-6ee7a035f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples.T[0], samples.T[1], color=\"orange\")\n",
    "plt.title(\"Two moon distribution\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd1696-bfa8-4e2e-9ff2-6665d3fa9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.from_numpy(samples).to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba1fdb-2614-4281-9fd6-698b0a2f8e6f",
   "metadata": {},
   "source": [
    "## Autoregressive Net and Transform\n",
    "\n",
    "The flow we implement below has affine autoregressive transforms. Most of the constructs are available in the `pyro` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7665839-de11-4f47-94ff-c5cb4abf5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2  # data dimension\n",
    "hidden_dims = [50*input_dim, 50*input_dim, 50*input_dim]\n",
    "\n",
    "base_dist = Normal(torch.zeros(input_dim), torch.ones(input_dim))\n",
    "\n",
    "arn = AutoRegressiveNN(input_dim, hidden_dims, param_dims=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90efbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a96393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two dimensional input -> mu and sigma (follow from lecture)\n",
    "arn(torch.ones(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd22b9-4f81-481f-8733-4e7a3da0843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform =  AffineAutoregressive(arn)  # the \"affine\" part implies the linear relation between hidden dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbbbc5-0687-40b7-89c0-0e028428b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the flow implementation is torch transformed distribution\n",
    "flow_dist = TransformedDistribution(base_dist, [transform])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae3f38",
   "metadata": {},
   "source": [
    "The `flow_dist` is the normalizing flow: It is a distribution which can be evaluated, and sampled from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03282fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_dist.sample([10]) # -> 10 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe94d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_points = torch.tensor(\n",
    "    [\n",
    "        [0., 0.],\n",
    "        [0., 1.],\n",
    "        [1., 0.],\n",
    "        [1., 1.],\n",
    "    ]\n",
    ")\n",
    "with torch.no_grad():\n",
    "    sample_log_prob = flow_dist.log_prob(sample_points)\n",
    "for sample, log_prob in zip(sample_points, sample_log_prob):\n",
    "    print(f\"log p({sample}) = {log_prob:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a7e07",
   "metadata": {},
   "source": [
    "# Now we train the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(transform.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4becbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def live_plot(x_vals, y_vals, iteration, labels=None):\n",
    "    \"\"\"Auxiliary function to visualize the distribution\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.scatter(x_vals, y_vals, label='proxy')\n",
    "    ax.scatter(samples.T[0], samples.T[1], alpha=0.1, label='Orig.', c=labels)\n",
    "    ax.legend()\n",
    "    ax.set_title('iteration {}'.format(iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddfb1e7-f099-4e5f-b85b-505eaf096e57",
   "metadata": {},
   "source": [
    "## Learn the Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895ba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 1000\n",
    "for i in range(num_iter):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # take the original samples, and evaluate the likelihood.\n",
    "    loss = -flow_dist.log_prob(samples).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    flow_dist.clear_cache()  # pyro modules cache values and derivatives for performance\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            samples_flow = flow_dist.sample(torch.Size([1000,])).numpy()\n",
    "        live_plot(samples_flow[:,0], samples_flow[:,1], i + 1)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fa6ac",
   "metadata": {},
   "source": [
    "## Compose several transforms\n",
    "\n",
    "In the previous case we just had a single transform. Now we compose several of those and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48082aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    AffineAutoregressive(\n",
    "        AutoRegressiveNN(\n",
    "            input_dim, hidden_dims,\n",
    "            param_dims=[1, 1]\n",
    "        )\n",
    "    ) for _ in range(5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caf996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_dist = TransformedDistribution(base_dist, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3862373-5671-41fb-af8f-c37a224b6bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = []\n",
    "\n",
    "for t in transforms:\n",
    "    trainable_parameters.extend(list(t.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(trainable_parameters, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a922df3-5165-4a5a-b2c1-3f62d0eb210d",
   "metadata": {},
   "source": [
    "### Learn the transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 5000\n",
    "for i in range(num_iter):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = -flow_dist.log_prob(samples).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    flow_dist.clear_cache()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        with torch.no_grad():\n",
    "            samples_flow = flow_dist.sample(torch.Size([1000,])).numpy()\n",
    "\n",
    "        live_plot(samples_flow[:,0], samples_flow[:,1], i + 1)\n",
    "        plt.xlim((-2.0, 3.0))\n",
    "        plt.ylim((-1.5, 1.5))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f6769-728c-4899-bca8-0e2af0502992",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parameters = lambda parameters: sum(p.numel() for p in parameters if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d9741-f914-42b7-935f-f25377a34d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainable parameters of single transform =\", num_parameters(transform.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c668e-0318-457a-8216-356055024ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainable parameters of after composing transforms =\", num_parameters(trainable_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103bb47-1d79-4275-be1e-4ed77d48e454",
   "metadata": {},
   "source": [
    "# Things to try\n",
    "\n",
    "- Compare results/number of trainable parameters from other flavors of autoregressive nets: splines, neural autoregressive etc.\n",
    "- Compare results/number of parameters with coupling layers instead. Note that like affine/spline autoregressive, there is the corresponding affine/spline coupling transforms.\n",
    "- Depending on whether the masked feed-forward layers are implemented from \"data\" to \"normal\" direction or opposite, the flow is called masked-autoregressive or inverse-autoregressive. Look at the `pyro` source code on github and infer which one is the above implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c2075-92ad-4554-82bf-c04e4046e746",
   "metadata": {},
   "source": [
    "# Sampling from each mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(samples.T[0], samples.T[1], c=labels)\n",
    "plt.title(\"Two colored moon\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244318db",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.from_numpy(labels).to(dtype=torch.float32).reshape(samples.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cc27d",
   "metadata": {},
   "source": [
    "Let's learn the conditional approximator based on the color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import ConditionalTransformedDistribution\n",
    "from pyro.nn.auto_reg_nn import ConditionalAutoRegressiveNN\n",
    "from pyro.distributions.transforms import ConditionalAffineAutoregressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb976281",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_dim = 1 # the color is either 0 or 1\n",
    "arn = ConditionalAutoRegressiveNN(input_dim, condition_dim, hidden_dims, param_dims=[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "arn(torch.ones(5, 2), context=torch.ones(5, 1))  # need to supply additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = [\n",
    "    ConditionalAffineAutoregressive(\n",
    "        ConditionalAutoRegressiveNN(\n",
    "            input_dim, condition_dim, hidden_dims,\n",
    "            param_dims=[1, 1]\n",
    "        )\n",
    "    ) for _ in range(5)\n",
    "]\n",
    "conditional_flow_dist = ConditionalTransformedDistribution(base_dist, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f9d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = []\n",
    "\n",
    "for t in transforms:\n",
    "    trainable_parameters.extend(list(t.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80786cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(trainable_parameters, lr=1e-3)\n",
    "num_iter = 1000\n",
    "\n",
    "for i in range(num_iter):\n",
    "    optimizer.zero_grad()\n",
    "    loss = -conditional_flow_dist.condition(labels).log_prob(samples).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    conditional_flow_dist.clear_cache()\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        inference_label = ((i + 1) // 100) % 2  # alternate between modes\n",
    "        with torch.no_grad():\n",
    "            samples_one = conditional_flow_dist.condition(\n",
    "                torch.tensor([1,])\n",
    "            ).sample(torch.Size([1000,])).numpy()\n",
    "\n",
    "        live_plot(samples_one[:,0], samples_one[:,1], i + 1, labels=labels)\n",
    "        plt.xlim((-2.0, 3.0))\n",
    "        plt.ylim((-1.5, 1.5))\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
