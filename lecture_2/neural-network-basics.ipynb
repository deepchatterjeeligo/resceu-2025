{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352a7a85",
   "metadata": {},
   "source": [
    "# Pytorch primitives\n",
    "\n",
    "Pytorch uses _tensors_ which are analogous to numpy arrays. Several API calls from numpy have drop-in replacements (though not always).\n",
    "\n",
    "A major difference is that tensors have an associated `device`, which is `cpu` by default, but can be a co-processor like `gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7019905-f9d5-4a94-9a44-b1486d7c103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa06d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 10)\n",
    "print(\"Tensor:\", x)\n",
    "print(\"Tensor device:\", x.device)\n",
    "print(\"In numpy: \", x.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16834a7",
   "metadata": {},
   "source": [
    "Another major difference compared to numpy arrays is gradient tracking. **Note**: this is a different concept compared to numerical calculation of gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(1, 10, 10, requires_grad=True)\n",
    "print(\"Value of x.grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f3ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the difference with np.gradient\n",
    "np_gradient = np.gradient(x.detach().numpy())\n",
    "print(\"Result of np.gradient:\", np_gradient) # this computes numerical gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b72c0",
   "metadata": {},
   "source": [
    "Tensor gradients are possible to calculate after an operations _that produces a scalar_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_scalar = x**2 # is not a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d47237",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = (x**2).sum() # is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb7e97",
   "metadata": {},
   "source": [
    "Gradients are computed using the `.backward()` call on a _scalar_. Here we have,\n",
    "\n",
    "\\begin{equation}\n",
    "    L = \\sum_{i=1}^{10} x_i^2\n",
    "\\end{equation}\n",
    "\n",
    "The gradient is calculated in the usual sense of partial derivativies:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial L}{\\partial x_k} = \\frac{\\partial}{\\partial x_k}\\left(\\sum_{i=1}^{10} x_i^2\\right) = 2 x_k\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107dabcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a2217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value of x.grad now: \", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd0c82",
   "metadata": {},
   "source": [
    "**Few points to note**\n",
    "\n",
    "- The gradient for the _scalar_ as opposed to the array.\n",
    "- It is _exact_ as opposed to being a numerical estimate.\n",
    "- We get it only at one point. If we change the value of $\\mathbf{x}$, we need to compute the scalar, $L$, again. Only then we get the gradient.\n",
    "- This is automatic differentiation: we know the value $\\mathbf{x}$ and derivative $\\partial L/\\partial x_a\\vert_{\\mathbf{x}}$ at a single point. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042f4a6",
   "metadata": {},
   "source": [
    "# Basics of Neural Networks\n",
    "\n",
    "- Neural networks are functions with parameters that can be tuned.\n",
    "- In their simplest form, they involve repeating linear transformations, followed by non-linear activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6103fc81",
   "metadata": {},
   "source": [
    "For example, $f: \\mathcal{R}^m \\rightarrow \\mathcal{R}^n$ with $\\bf{y} = f(\\bf{x})$ is a neural net\n",
    "\n",
    "\\begin{equation}\n",
    "y_j = \\sum_{k=1}^{p} A_{jk}\\;\\sigma\\;\\left(\\sum_{j=1}^{m} W_{ik}\\;x_k\\right) + B_j\n",
    "\\\\\n",
    "\\text{   where, }\\sigma(x) = \\frac{1}{1 + \\exp(x)}\n",
    "\\end{equation}\n",
    "\n",
    "There is a linear transformation of $\\bf{x}$ using matrix $\\bf{W}$, followed by passing the result to a squeezer function $\\sigma(x)$, followed by another linear transformation using matrix $\\bf{A}$ and vector $\\bf{B}$.\n",
    "\n",
    "- $\\bf{W}$ has $(m \\times p)$ tunable parameters\n",
    "- $\\sigma$ has none\n",
    "- $\\bf{A}$ has $(n \\times p)$ tunable parameters\n",
    "- $\\bf{B}$ has $n$ tunable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecfd952",
   "metadata": {},
   "source": [
    "### An example with $m = n = 20; p = 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe05f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = n = 20\n",
    "p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0259c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = lambda x: x**3 - x  # function to approximate\n",
    "\n",
    "x_vals = torch.linspace(-1, 1, m)\n",
    "y_vals = poly(x_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1d79a-898b-4b5c-b309-c52eb7f5d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba88c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = torch.nn.Linear(m, p, bias=False)\n",
    "        self.a_b = torch.nn.Linear(p, n, bias=True) \n",
    "\n",
    "    def forward(self, r):\n",
    "        r = torch.sigmoid(self.w(r))\n",
    "        r = self.a_b(r)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc6949",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab5f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trainable_params = 0\n",
    "print(\"Trainable Parameters:\")\n",
    "for name, param in net.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        total_trainable_params += param.numel()\n",
    "\n",
    "print(f\"Total Trainable Parameters: {total_trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9579cbd",
   "metadata": {},
   "source": [
    "Plot the NN output before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb7e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_vals, y_vals, label='True function', linestyle='dashed')\n",
    "plt.plot(x_vals, net(x_vals).detach(), label='NN before training')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary function to create a live plot\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def live_plot(x_vals, y_vals, y_pred, epoch, loss_val):\n",
    "    \"\"\"Auxiliary function to visualize the distribution\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    sleep(1)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    ax.plot(x_vals, y_vals, label='True function', linestyle='dashed')\n",
    "    ax.plot(x_vals, y_pred)\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Epoch {epoch} ; Loss = {loss_val:.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a68bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)  # Using Adam optimizer\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # pass input through the NN\n",
    "    y_pred = net(x_vals)\n",
    "\n",
    "    # compute the scalar loss\n",
    "    loss = criterion(y_pred, y_vals)\n",
    "    # then evaluate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # adjust parameters using gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        #print(\"Epoch {}: Loss = {:.4e}\".format(epoch, loss.detach().numpy()))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred_train = net(x_vals)\n",
    "        live_plot(x_vals, y_vals, y_pred_train, epoch, loss.detach().numpy())\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
